---
title: "Homework_3"
author: "Kodiak Soled"
date: "10/11/2019"
output: github_document
always_allow_html: yes
---

# Prework

_Note to TAs: you may need to_ `install.packages(kableExtra)` _to run my code._

### First, we need to load the packages we will need for this homework assignment as well as Jeff's favorite settings that will be applied to this entire R Markdown document:

```{r}
library(viridis)
library(tidyverse)
library(kableExtra)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d

scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

# Problem 1 

## Instacart Dataset

### We first need to load the data from the `p8105.datasets` and tidy the dataset before we can begin exploring it: 

```{r}
library(p8105.datasets)
data("instacart")

cleaned_instacart = 
  instacart %>%
  janitor::clean_names() %>%
  mutate(product_name = str_to_lower(product_name))
```

### We can use the `str` function to compactly display the internal structure of the data and begin exploring its size and structure: 

```{r}
str(cleaned_instacart)
```

### Description 

* We can see that there are `r nrow(instacart)` observations of `r ncol(instacart)` variables for a total of `r nrow(instacart)*ncol(instacart)` values in the Instacart dataset. The majority of the variables are integers, but four are character vectors (`department`, `aisle`, `product_name`, and `eval_set`). 
* Some key variables in this dataset include the order and product identifier, the name of the product, the name and identifier of the department and aisle, and several variables that include information about the ordering history of each product. 
* An illustrative example in this dataset is that an "organic hass avocado" (product identifier #47209) was purchased by customer #112108 at 10 am on the 4th day of the week. This produce is located in the "fresh fruit" aisle (aisle identifier #24) which is part of the "produce" department (department identifier #4). In total, there were `r sum(pull(cleaned_instacart %>% filter(product_name == "organic hass avocado") %>% count()))` organic hass avocados ordered in this dataset. Another way of looking at this dataset is that customer #112108 placed an order at 10 am (order identifier #1) which had eight items in it ("bulgarian yogurt", "organic 4% milk fat whole milk cottage cheese", "organic celery hearts", "cucumber kirby", "lightly smoked sardines in olive oil", "bag of organic bananas", "organic hass avocado", and "organic whole string chees"e) which came from three departments ("dairy eggs", "produce", and "canned goods").

## Problem 1 Questions

### First, we can determine the number of aisles in the Instacart dataaset by grouping the dataset by aisle (`group_by`) then counting the number of aisles (`count`). We can also determine which aisles most items are ordered from by arranging the data in descending order (`arrange(desc(n))`):

```{r}
aisle = 
  cleaned_instacart %>% 
  group_by(aisle) %>%
  count() %>%
  arrange(desc(n)) 
aisle
```

### Description
* From this tibble, we can see there are `r nrow(aisle)` aisles in this dataset. The most orders are from the fresh vegetables and fresh fruits aisles with 150,609 and 150,473 orders from each aisle, respectively.

### Next we can make a plot using `geom_bar` within `ggplot` that shows the number of items ordered in each aisle, for aisles with more than 10,000 items ordered by using the `filter` function. We can take a few more steps so that the aisles are arranged sensibly by reordering the aisles using the `forcats::fct_reorder` function and flipping the x and y axis so it is easier to read the names of the aisles using `coord_flip`:

```{r}
cleaned_instacart %>%
  count(aisle) %>%
  filter(n > 10000) %>%
  mutate(aisle = forcats::fct_reorder(aisle, n, .desc = TRUE)) %>%
  ggplot(aes(x = aisle, y = n)) +
  geom_bar(stat = "identity") + coord_flip() +
  labs(
    title = "Number of Items Ordered per Aisles (>10,000)",
    x = "Number of Items Ordered",
    y = "Aisle Name", 
    caption = "Data from the Instacart Dataset"
    )
```

### Description
* It is also evident from this bar plot that the "fresh vegetables" and "fresh fruits" aisles are clearly the most popular aisles to order from.

### To make a table with the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”, we need to perform several steps. We need to:
* Reduce our dataframe by first `select`ing the rows we care about ("aisle" and "product_name") then `filter`ing so only the three aisles we care about are present in the dataset
* Count the number of times each product was ordered using `count`, group the products by the aisle they are from using `group_by`, then select the top three products in each aisle using `top_n`
* Create a rank varaible to specify the popularity of each product from each aisle using `rank(desc(n))`
* Organize the dataset for increased ease of reading by `arrange`ing the products in each aisle in descending order
* Produce our reader-friendly table using `knitr::kable`:

```{r}
cleaned_instacart %>%
  select(aisle, product_name) %>%
  filter(
    aisle == "baking ingredients" |
      aisle == "packaged vegetables fruits" |
      aisle == "dog food care"
    ) %>%
  count(product_name, aisle) %>%
  group_by(aisle) %>%
  top_n(n = 3) %>%
  mutate(
    rank(desc(n))
    ) %>%
  rename(rank = 'rank(desc(n))') %>%
  select(aisle, product_name, n, rank) %>%
  arrange(aisle, desc(n)) %>%
  knitr::kable(caption = "Three Most Popular Items among the Baking Ingredients, Packaged Vegetable Fruits, and Dog Food Car Aisles") %>%
  kable_styling(bootstrap_options = c("striped", "condensed", font_size = 12))
```

### Lastly, we can make a table of the mean hour of the day that "Pink Lady Apples" and "Coffee Ice Cream" were ordered on each day of the week by the following steps:
* Reduce our dataframe using `filter` so "Pink Lady Apples" and "Coffee Ice Cream" are the only products present, then using `select` so "order_dow", "product_name", and "order_hour_of_day" are the only variables present.
* Organize the dataset by: renaming "order_dow" to "day_of_week" using `mutate`, recoding the days of the week from 0-6 to Sunday-Saturday using `recode`, and reordering the days of the week from "monday" to "sunday" using `forcats::fct_relevel`
* Obtain the mean hour of the day each product was ordered by first grouping the product and the day of the week it was ordered, then using `sumarize` to compute the mean hour of the day each product ("Pink Lady Apples" and "Coffee Ice Cream") was ordered on each day of the week (monday-sunday)
* Organize the dataset to make more reader friendly by pivoting the table from long to wide using `pivot_wider` so the variable names are the two products ("Pink Lady Apples" and "Coffee Ice Cream"), the rows are the days of the week (monday-sunday), and the values are the mean time of day each product was ordered
* Produce our reader-friendly table using `knitr::kable`:

```{r}
cleaned_instacart %>%
  filter(
    product_name == "pink lady apples" | 
      product_name == "coffee ice cream"
    ) %>%
  select(order_dow, product_name, order_hour_of_day) %>%
  mutate(
    day_of_week = recode(order_dow,
                         `1` = "monday", 
                         `2` = "tuesday", 
                         `3` = "wednesday", 
                         `4` = "thursday", 
                         `5` = "friday", 
                         `6` = "saturday",
                         `0` = "sunday"),
    day_of_week = forcats::fct_relevel(day_of_week, c("monday", "tuesday", "wednesday", "thursday", "friday", "saturday", "sunday"))
    ) %>%
  group_by(product_name, day_of_week) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  pivot_wider(
    names_from = "product_name",
    values_from = "mean_hour"
  ) %>%
   knitr::kable(digit = 1, caption = "Mean Hour of Day Pink Lady Apples and Coffee Ice Cream is Ordered Each Day of Week") %>% 
  kable_styling(bootstrap_options = c("striped", "condensed", font_size = 12))
```

* We can see from this table that both coffee ice cream and pink lady apples orders are on average placed between 11 am and 4 pm. Cofee ice cream also tends to be ordered later in the da than pink lady apples.

# Problem 2

## BRFSS Dataset

### We first need to load the brfss data from the `p8105.datasets`:

```{r}
library(p8105.datasets)
data("brfss_smart2010")
```

### Then we can clean the data as instructed: 
* Use `janitor::clean_names` to tidy the data and `mutate_all` to change all variables and values to lower case
* `rename` a few variables ("locationabbr", "locationdesc", and "respid") to have more appropriate variable names
* Focus on the "overall health" topic by `filter`ing for that variable (we don't need to `filter` again to only include responses from “excellent” to “poor” as `filter`ing for "overall health" already limited the response values from “excellent” to “poor”)
* Organize responses from "poor" to "excellent" using `forcats::fct_relevel` within the `mutate` function

```{r}
brfss =
  brfss_smart2010 %>%
  janitor::clean_names() %>%
  mutate_all(tolower) %>%
  rename(
    state = locationabbr, 
    location = locationdesc,
    response_id = respid
    ) %>%
  filter(topic == "overall health") %>%
  mutate(
    response = forcats::fct_relevel(response, c("poor", "fair", "good", "very good", "excellent"))
    )
```

## Problem 2 Questions

### To determine which states were observed at 7 or more locations in 2002 and in 2010, we need to perform the following steps for each year: 
* `filter` by the year
* `group_by` the "state" then `summarize` the "locations" that are unique using `n_distinct`
* `filter` for states that were observed at 7 or more locations

```{r}
states_2002_locations = 
  brfss %>%
  filter(year == 2002) %>%
  group_by(state) %>%
  summarize(num_location = n_distinct(location)) %>%
  filter(num_location >= 7)

states_2002_locations %>%
   knitr::kable(caption = "States Observed in >=7 Locations in 2002") %>% 
  kable_styling(bootstrap_options = c("striped", "condensed", font_size = 12))

states_2010_locations =
  brfss %>%
  filter(year == 2010) %>%
  group_by(state) %>%
  summarize(location = n_distinct(location)) %>%
  filter(location >= 7) 

states_2010_locations %>%
   knitr::kable(caption = "States Observed in >=7 Locations in 2002") %>% 
  kable_styling(bootstrap_options = c("striped", "condensed", font_size = 12))
```

### Description
* There were `r nrow(states_2002_locations)` states in 2002 that were observed at 7 or more locations. These states included: ct, ft, ma, nc, nj, pa.

* There were `r nrow(states_2010_locations)` states in 2010 that were observed at 7 or more locations. These states included: ca, co, fl, ma, md, nc, ne, nj, ny, oh, pa, sc, tx, wa.

### To make a dataset that is limited to "excellent" responses, and contains, year, state, and a variable "mean_data_value" that averages the "data_value" across locations within a state we need to do the following:
* Reduce the dataframe by first `filter`ing the responses that are "excellent", then `select`ing the variables that we care about ("year", "state", and "data_value")
* `mutate` the variable "data_value" from a character to numeric vector in preparation of finding the mean of this variable
* Create the "mean_data_value" variable by first grouping by "state" and "year", then using `summarize` to create a new varaible that is the average "data_value" across all locations within a state:

```{r}
excellent_brfss =
  brfss %>%
  filter(response == "excellent") %>%
  select(year, state, data_value) %>%
  mutate(data_value = as.numeric(data_value)) %>%
  group_by(state, year) %>%
  summarize(mean_data_value = mean(data_value))

excellent_brfss
```

### We can see the summary statistics on the resulting dataset by using the `summary` function: 

```{r}
summary(excellent_brfss)
```

### Description
* There are `r nrow(excellent_brfss)` rows in this resulting dataset of `r ncol(excellent_brfss)` variables. The "mean_data_value" variable we created ranges from 11.50 to 29.46 with an average value of 21.97.

### Now from this dataset, we can make a “spaghetti” plot of the `mean_data_value` over time within a state. This requires we plot "year" on the x-axis, "mean_data_value" (i.e., "mean prevalence") on the y-axis, and then `group` by "state" to produce a plot (using `geom_line`) that has a line for each state which shows the mean prevalence of the state from 2002 to 2010:

```{r}
ggplot(excellent_brfss, aes(x = year, y = mean_data_value, group = state, color = state)) +
  geom_line() + 
  labs(
    title = "Prevalence of Adult US Residents by State from 2002-2010",
    x = "Year",
    y = "Mean Prevalence", 
    caption = "Data from the BRFSS Dataset")
```

### Description
* This plot visually displays what we described above: the mean prevalence ranges from ~12-30 with an average value of ~23. We can also see that west virginia (wv) has a big dip in 2005 to 11.5 and another dip in 2009 to 13.4. This may be pulling down the national mean prevelance of states with "excellent" responses and we may want to investigate what happened in west virginia in those two years. 

### Lastly, to can make a two-panel plot showing the distribution of `data_value` for responses (“poor” to “excellent”) among locations in NY state for the years 2006 and 2010 we need to do the following steps: 
* `filter` the dataset for the years "2010" and "2006", as well as for the state of "ny"
* `select` the other variables we will want in our dataset ("year", "location", "data_value", "response") and leave out the remaining variables
* `mutate` the variable "data_value" from a character to numeric vector in preparation of plotting this variable on the y-axis
* Plot our responses ("poor" to "excellent") on our x-axis, and our numeric "data_value" on the y-axis using `geom_boxplot` so we can visualize the distribution of `data_value`
* Use `facet_grid` to create a two-panel plot for the years 2006 and 2010
* Make the table more reader-friendly with the `labs` function so we can add a title and captions

```{r}
brfss %>%
  filter(state == "ny", 
         year == 2010 |
           year == 2006
         ) %>%
  select(year, location, data_value, response) %>%
  mutate(
    data_value = as.numeric(data_value),
    ) %>%
  ggplot(aes(x = response, y = data_value)) +
  geom_boxplot() +
  facet_grid(. ~year) + 
  labs(
    title = "Quality of General Health of Adult US Residents among locations in NY in 2006 and 2010",
    x = "Quality of General Health",
    y = "Prevalence",
    caption = "Data from the BRFSS Dataset")
```

### Description
* From these two side-by-side plots, we can visualize how the quality of health of NY residents varied between the years of 2006 and 2010. It seems that quality of health became more extreme in 2010 where residents' ratings of poor and fair health were lower than in 2006 and their ratings of good, very good, and excellent health were higher than in 2006

# Problem 3 

## Accelerometer Data

### We first need to read in the Accelerometer data and tidy it according to the instructions:
* Include all originally observed variables and values
* Have useful variable names
* Make a weekday vs weekend variable
* Create reasonable variable classes

### To achieve this we can use the following process: 
* Clean up the dataset using `janitor::clean_names`
* Round off the values of the variables from "activity_1" to "activity_1440" using the `round` function in `mutate_at`
* Use `mutate` to: make the "day" variable lower case; make a "weekday" variable for monday-friday and a "weekend" variable for saturday and sunday using `case_when`; and order the data so the days of the week display from monday to sunday using `forcats::fct_relevel`
* Order the variables so that it displays in the order of: "week", "day_id", "day", "day_type", then everything else using `select`
* `arrange` the values of "week" and "day so they are first ordered by week (1-5) then by day (monday-sunday)
* Lastly, to create a tidy dataset, we can pivot the dataframe from wide to long using `pivot_longer` so that each "minute_of_day" had its own row with a cooresponding "activity_count":

```{r}
accel_data = 
  read_csv("./data/accel_data.csv") %>%
  janitor::clean_names() %>%
  mutate_at(vars(starts_with("activity")), funs(round(., 1))) %>%
  mutate(
    day = str_to_lower(day),
    day_type = case_when(
      day %in% c("monday", "tuesday", "wednesday", "thursday", "friday") ~ "weekday",
      day %in% c("saturday", "sunday") ~ "weekend", 
      TRUE ~ ""
      ),
    day = forcats::fct_relevel(day, c("monday", "tuesday", "wednesday", "thursday", "friday", "saturday", "sunday"))
    ) %>%
  select(week, day_id, day, day_type, everything()) %>%
  arrange(week, day) %>%
  pivot_longer(
    cols = starts_with("activity_"),
    names_to = "minute_of_day",
    names_prefix = "activity_",
    values_to = "activity_count"
  )

accel_data
```

### Description
* There are `r nrow(accel_data)` observations of `r ncol(accel_data)` variables in the Acceleration dataset. The key variables include: "week" (1-5), "day_id" (1-35), "day" (monday-sunday), "type of day" (weekday/weekend), "minute_of_day" (1-1440), and each minute's corresponding "activity_count". 

### From this tidied dataset, we can now aggregate the data accross minutes to create a total activity variable for each day. We can do this by grouping the dataset by "week", "day", and "day_type" ("day_id" was dropped as it wasn't deemed meaningful to include for this table). Then, we can summarize the "activity_count" using the `sum` function. We can then create a reader-friendly table to display the "total_activity" for each of the 35 days using `knitr::kable`:

```{r}
aggregate_accel_data =
  accel_data %>%
  group_by(week, day, day_type) %>%
  summarize(total_activity = sum(activity_count))

aggregate_accel_data %>%
    knitr::kable(caption = "Total Activity (mean minuutes) Count in a 24-Hour Period for Five Weeks") %>%
  kable_styling(bootstrap_options = c("striped", "condensed", font_size = 12))
```

## Identifying trends in the aggregated dataset

### We can create a few visuals of the aggregated dataset in order to identify trends that are occuring: 

### 1. We can create a line plot to examine the average (`mean(activity_count` for an easier-to-read y-axis)) activity count for each of the five weeks by the day of the week: 

```{r}
accel_data %>%
  group_by(week, day, day_type) %>%
  summarize(average_activity = mean(activity_count)) %>%
  ggplot(aes(x = day, y = average_activity, group = week, color = week)) +
  geom_line() +
  labs(
    title = "Physicial Activity of a 63-year-old Male \nAcross Five Weeks by Day of the Week",
    x = "Day of the Week",
    y = "Activity Counts",
    caption = "Data from the Accelerometer Dataset")
```

* This plot allows us to see that this man's activity increased across the week (from monday to sunday) for weeks 1, 2, and 3. However, for weeks 4 and 5, there is a sharp decline in activity that occurs over the weekend compared to the weekday.

### 2. We can create another line plot to examine the average (`mean(activity_count` for an easier-to-read y-axis)) activity count for each of day of the week across the five weeks: 

```{r}
accel_data %>%
  group_by(week, day, day_type) %>%
  summarize(average_activity = mean(activity_count)) %>%
  ggplot(aes(x = week, y = average_activity, group = day, color = day)) +
  geom_line() +
  labs(
    title = "Physicial Activity of a 63-year-old Male \nAcross the Days of the Week over Five Weeks",
    x = "Week",
    y = "Activity Counts",
    caption = "Data from the Accelerometer Dataset")
```

* This second plot allows us to see that this man's activity trended towards decresing from week 1 to week 5, particularly between week 3 and week 4 and also declines over the weekend. 
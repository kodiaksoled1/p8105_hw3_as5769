---
title: "Homework_3"
author: "Kodiak Soled"
date: "10/11/2019"
output: github_document
always_allow_html: yes
---

# Prework

_Note to TAs: you may need to_ `install.packages(kableExtra)` _to run my code._

### First, we need to load the packages we will need for this homework assignment as well as Jeff's favorite settings that will be applied to this entire R Markdown document:

```{r}
library(viridis)
library(tidyverse)
library(kableExtra)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d

scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

# Problem 1 

## Instacart Dataset

### We first need to load the data from the `p8105.datasets` and tidy the dataset before we can begin exploring it: 

```{r}
library(p8105.datasets)
data("instacart")

cleaned_instacart = 
  instacart %>%
  janitor::clean_names() %>%
  mutate(product_name = str_to_lower(product_name))
```

### We can use the `str` function to compactly display the internal structure of the data and begin exploring its size and structure: 

```{r}
str(cleaned_instacart)
```

### Description 

* We can see that there are `r nrow(instacart)` observations of `r ncol(instacart)` variables for a total of `r nrow(instacart)*ncol(instacart)` values in the Instacart dataset. The majority of the variables are integers, but four are character vectors (`department`, `aisle`, `product_name`, and `eval_set`). 
* Some key variables in this dataset include the order and product identifier, the name of the product, the name and identifier of the department and aisle, and several variables that include information about the ordering history of each product. 
* An illustrative example in this dataset is that an "organic hass avocado" (product identifier #47209) was purchased by customer #112108 at 10 am on the 4th day of the week. This produce is located in the "fresh fruit" aisle (aisle identifier #24) which is part of the "produce" department (department identifier #4). In total, there were `r sum(pull(cleaned_instacart %>% filter(product_name == "organic hass avocado") %>% count()))` organic hass avocados ordered in this dataset. Another way of looking at this dataset is that customer #112108 placed an order at 10 am (order identifier #1) which had eight items in it ("bulgarian yogurt", "organic 4% milk fat whole milk cottage cheese", "organic celery hearts", "cucumber kirby", "lightly smoked sardines in olive oil", "bag of organic bananas", "organic hass avocado", and "organic whole string chees"e) which came from three departments ("dairy eggs", "produce", and "canned goods").

## Problem 1 Questions

### First, we can determine the number of aisles in the Instacart dataaset by grouping the dataset by aisle (`group_by`) then counting the number of aisles (`count`). We can also determine which aisles most items are ordered from by arranging the data in descending order (`arrange(desc(n))`):

```{r}
aisle = 
  cleaned_instacart %>% 
  group_by(aisle) %>%
  count() %>%
  arrange(desc(n)) 
aisle
```

### Description
* From this tibble, we can see there are `r nrow(aisle)` aisles in this dataset. The most orders are from the fresh vegetables and fresh fruits aisles with 150,609 and 150,473 orders from each aisle, respectively.

### Next we can make a plot using `geom_bar` within `ggplot` that shows the number of items ordered in each aisle, for aisles with more than 10,000 items ordered by using the `filter` function. We can take a few more steps so that the aisles are arranged sensibly by reordering the aisles using the `forcats::fct_reorder` function and flipping the x and y axis so it is easier to read the names of the aisles using `coord_flip`:

```{r}
cleaned_instacart %>%
  count(aisle) %>%
  filter(n > 10000) %>%
  mutate(aisle = forcats::fct_reorder(aisle, n, .desc = TRUE)) %>%
  ggplot(aes(x = aisle, y = n)) +
  geom_bar(stat = "identity") + coord_flip() +
  labs(
    title = "Number of Items Ordered per Aisles (>10,000)",
    x = "Number of Items Ordered",
    y = "Aisle Name", 
    caption = "Data from the Instacart Dataset"
    )
```

### Description
* It is also evident from this bar plot that the "fresh vegetables" and "fresh fruits" aisles are clearly the most popular aisles to order from.

### To make a table with the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”, we need to perform several steps. We need to:
* Reduce our dataframe by first `select`ing the rows we care about ("aisle" and "product_name") then `filter`ing so only the three aisles we care about are present in the dataset
* Count the number of times each product was ordered using `count`, group the products by the aisle they are from using `group_by`, then select the top three products in each aisle using `top_n`
* Create a rank varaible to specify the popularity of each product from each aisle using `rank(desc(n))`
* Organize the dataset for increased ease of reading by `arrange`ing the products in each aisle in descending order
* Produce our reader-friendly table using `knitr::kable`:

```{r}
cleaned_instacart %>%
  select(aisle, product_name) %>%
  filter(
    aisle == "baking ingredients" |
      aisle == "packaged vegetables fruits" |
      aisle == "dog food care"
    ) %>%
  count(product_name, aisle) %>%
  group_by(aisle) %>%
  top_n(n = 3) %>%
  mutate(
    rank(desc(n))
    ) %>%
  rename(rank = 'rank(desc(n))') %>%
  select(aisle, product_name, n, rank) %>%
  arrange(aisle, desc(n)) %>%
  knitr::kable(caption = "Three Most Popular Items among the Baking Ingredients, Packaged Vegetable Fruits, and Dog Food Car Aisles") %>%
  kable_styling(bootstrap_options = c("striped", "condensed", font_size = 12))
```

### Lastly, we can make a table of the mean hour of the day that "Pink Lady Apples" and "Coffee Ice Cream" were ordered on each day of the week by the following steps:
* Reduce our dataframe using `filter` so "Pink Lady Apples" and "Coffee Ice Cream" are the only products present, then using `select` so "order_dow", "product_name", and "order_hour_of_day" are the only variables present.
* Organize the dataset by: renaming "order_dow" to "day_of_week" using `mutate`, recoding the days of the week from 0-6 to Sunday-Saturday using `recode`, and reordering the days of the week from "monday" to "sunday" using `forcats::fct_relevel`
* Obtain the mean hour of the day each product was ordered by first grouping the product and the day of the week it was ordered, then using `sumarize` to compute the mean hour of the day each product ("Pink Lady Apples" and "Coffee Ice Cream") was ordered on each day of the week (monday-sunday)
* Organize the dataset to make more reader friendly by pivoting the table from long to wide using `pivot_wider` so the variable names are the two products ("Pink Lady Apples" and "Coffee Ice Cream"), the rows are the days of the week (monday-sunday), and the values are the mean time of day each product was ordered
* Produce our reader-friendly table using `knitr::kable`:

```{r}
cleaned_instacart %>%
  filter(
    product_name == "pink lady apples" | 
      product_name == "coffee ice cream"
    ) %>%
  select(order_dow, product_name, order_hour_of_day) %>%
  mutate(
    day_of_week = recode(order_dow,
                         `1` = "monday", 
                         `2` = "tuesday", 
                         `3` = "wednesday", 
                         `4` = "thursday", 
                         `5` = "friday", 
                         `6` = "saturday",
                         `0` = "sunday"),
    day_of_week = forcats::fct_relevel(day_of_week, c("monday", "tuesday", "wednesday", "thursday", "friday", "saturday", "sunday"))
    ) %>%
  group_by(product_name, day_of_week) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  pivot_wider(
    names_from = "product_name",
    values_from = "mean_hour"
  ) %>%
   knitr::kable(digit = 1, caption = "Mean Hour of Day Pink Lady Apples and Coffee Ice Cream is Ordered Each Day of Week") %>% 
  kable_styling(bootstrap_options = c("striped", "condensed", font_size = 12))
```

* We can see from this table that both coffee ice cream and pink lady apples orders are on average placed between 11 am and 4 pm. Cofee ice cream also tends to be ordered later in the da than pink lady apples.

# Problem 2

## BRFSS Dataset

### We first need to load the brfss data from the `p8105.datasets`:

```{r}
library(p8105.datasets)
data("brfss_smart2010")
```

### Then we can clean the data as instructed: 
* Use `janitor::clean_names` to tidy the data and `mutate_all` to change all variables and values to lower case
* `rename` a few variables ("locationabbr", "locationdesc", and "respid") to have more appropriate variable names
* Focus on the "overall health" topic by `filter`ing for that variable (we don't need to `filter` again to only include responses from “excellent” to “poor” as `filter`ing for "overall health" already limited the response values from “excellent” to “poor”)
* Organize responses from "poor" to "excellent" using `forcats::fct_relevel` within the `mutate` function

```{r}
brfss =
  brfss_smart2010 %>%
  janitor::clean_names() %>%
  mutate_all(tolower) %>%
  rename(
    state = locationabbr, 
    location = locationdesc,
    response_id = respid
    ) %>%
  filter(topic == "overall health") %>%
  mutate(
    response = forcats::fct_relevel(response, c("poor", "fair", "good", "very good", "excellent"))
    )
```

## Problem 2 Questions

### To determine which states were observed at 7 or more locations in 2002 and in 2010, we need to perform the following steps for each year: 
* `filter` by the year
* `group_by` the "state" then `summarize` the "locations" that are unique using `n_distinct`
* `filter` for states that were observed at 7 or more locations

```{r}
states_2002_locations = 
  brfss %>%
  filter(year == 2002) %>%
  group_by(state) %>%
  summarize(num_location = n_distinct(location)) %>%
  filter(num_location >= 7)

states_2002_locations %>%
   knitr::kable(caption = "States Observed in >=7 Locations in 2002") %>% 
  kable_styling(bootstrap_options = c("striped", "condensed", font_size = 12))

states_2010_locations =
  brfss %>%
  filter(year == 2010) %>%
  group_by(state) %>%
  summarize(location = n_distinct(location)) %>%
  filter(location >= 7) 

states_2010_locations %>%
   knitr::kable(caption = "States Observed in >=7 Locations in 2002") %>% 
  kable_styling(bootstrap_options = c("striped", "condensed", font_size = 12))
```

### Description
* There were `r nrow(states_2002_locations)` states in 2002 that were observed at 7 or more locations. These states included: ct, ft, ma, nc, nj, pa.

* There were `r nrow(states_2010_locations)` states in 2010 that were observed at 7 or more locations. These states included: ca, co, fl, ma, md, nc, ne, nj, ny, oh, pa, sc, tx, wa.

### To make a dataset that is limited to "excellent" responses, and contains, year, state, and a variable "mean_data_value" that averages the "data_value" across locations within a state we need to do the following:
* Reduce the dataframe by first `filter`ing the responses that are "excellent", then `select`ing the variables that we care about ("year", "state", and "data_value")
* `mutate` the variable "data_value" from a character to numeric vector in preparation of finding the mean of this variable
* Create the "mean_data_value" variable by first grouping by "state" and "year", then using `summarize` to create a new varaible that is the average "data_value" across all locations within a state:

```{r}
excellent_brfss =
  brfss %>%
  filter(response == "excellent") %>%
  select(year, state, data_value) %>%
  mutate(data_value = as.numeric(data_value)) %>%
  group_by(state, year) %>%
  summarize(mean_data_value = mean(data_value))

excellent_brfss
```